{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Contextual Retrieval based RAG System"
      ],
      "metadata": {
        "id": "jbw4wHV4zlKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "4vtFl39Ofu_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain #==0.3.10\n",
        "!pip install langchain-openai #==0.2.12\n",
        "!pip install langchain-community #==0.3.11"
      ],
      "metadata": {
        "id": "LVX6450Lfu_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install jq #==1.8.0"
      ],
      "metadata": {
        "id": "YikT3_10w1PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pymupdf #==1.25.1"
      ],
      "metadata": {
        "id": "fUdOiWH6w4GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Chroma Vector DB and LangChain wrapper"
      ],
      "metadata": {
        "id": "bwUBYHjPfu_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain-chroma #==0.1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e03a0839-baf8-47e0-afb4-1b7808d00749",
        "id": "p30SmCgTfu__"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-0.2.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.70 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (0.3.75)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (2.0.2)\n",
            "Collecting chromadb>=1.0.9 (from langchain-chroma)\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.17.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.25.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (0.4.23)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.70->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.32.5)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (0.24.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma) (0.57b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n",
            "Downloading langchain_chroma-0.2.5-py3-none-any.whl (12 kB)\n",
            "Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=96990f8a8f76d257c1d78bd7e398c04f8cd3691cd1cd86b5e82fc2966200e91b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb, langchain-chroma\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.20 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 langchain-chroma-0.2.5 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "opentelemetry"
                ]
              },
              "id": "34239ee283314610abf8736d6801fcf5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "EITC17hwfu__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "yEh2olNvfvAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "jiokYxD8fvAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "-On4AS0HfvAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Processing the Data"
      ],
      "metadata": {
        "id": "afzeN_WkHIz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the dataset"
      ],
      "metadata": {
        "id": "RA_-hzHbFeSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you can't download using the following code\n",
        "# go to https://drive.google.com/file/d/1aZxZejfteVuofISodUrY2CDoyuPLYDGZ download it\n",
        "# manually upload it on colab\n",
        "!gdown 1aZxZejfteVuofISodUrY2CDoyuPLYDGZ"
      ],
      "metadata": {
        "id": "RZFMYH-yFhWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67830b76-9533-4932-f615-9dc229b61802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aZxZejfteVuofISodUrY2CDoyuPLYDGZ\n",
            "To: /content/rag_docs.zip\n",
            "100% 5.92M/5.92M [00:00<00:00, 22.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip rag_docs.zip"
      ],
      "metadata": {
        "id": "WwLEBC4nF9ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50098f00-4400-4a3b-bec7-2f7b28807091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  rag_docs.zip\n",
            "   creating: rag_docs/\n",
            "  inflating: rag_docs/attention_paper.pdf  \n",
            "  inflating: rag_docs/cnn_paper.pdf  \n",
            "  inflating: rag_docs/resnet_paper.pdf  \n",
            "  inflating: rag_docs/vision_transformer.pdf  \n",
            "  inflating: rag_docs/wikidata_rag_demo.jsonl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Process JSON Documents"
      ],
      "metadata": {
        "id": "wMlxKZ_5jIdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import JSONLoader\n",
        "\n",
        "loader = JSONLoader(file_path='./rag_docs/wikidata_rag_demo.jsonl',\n",
        "                    jq_schema='.',\n",
        "                    text_content=False,\n",
        "                    json_lines=True)\n",
        "wiki_docs = loader.load()"
      ],
      "metadata": {
        "id": "RZ5y0NfzHPhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs)"
      ],
      "metadata": {
        "id": "G4E1zYFSG7J-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6f267e-d73f-4ffb-f6f9-a0ce28de20ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1801"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_docs[3]"
      ],
      "metadata": {
        "id": "aSbhERAyGw0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5002b4-88f0-4fde-a412-ead907586ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 4}, page_content='{\"id\": \"71548\", \"title\": \"Chi-square distribution\", \"paragraphs\": [\"In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\\\u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution.\", \"Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\"]}')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.docstore.document import Document\n",
        "wiki_docs_processed = []\n",
        "\n",
        "for doc in wiki_docs:\n",
        "    doc = json.loads(doc.page_content)\n",
        "    metadata = {\n",
        "        \"title\": doc['title'],\n",
        "        \"id\": doc['id'],\n",
        "        \"source\": \"Wikipedia\",\n",
        "        \"page\": 1\n",
        "    }\n",
        "    data = ' '.join(doc['paragraphs'])\n",
        "    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"
      ],
      "metadata": {
        "id": "yICyAF85h2DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_docs_processed[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IATrHWKh7II",
        "outputId": "799171fb-be7f-4ecf-9eae-339cf770479e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'title': 'Chi-square distribution', 'id': '71548', 'source': 'Wikipedia', 'page': 1}, page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\xa0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution. Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Process PDF documents"
      ],
      "metadata": {
        "id": "F_GzvHP1jSBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Chunk Contexts for Contextual Retrieval\n",
        "\n",
        "![](https://i.imgur.com/LRhKHzk.png)"
      ],
      "metadata": {
        "id": "4vH6xGFOnv7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "jxHHyhlbl_9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from the LangChain framework and standard libraries\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser, Document\n",
        "\n",
        "# --- Helper Function to Generate Context for a Single Chunk ---\n",
        "\n",
        "def generate_chunk_context(document, chunk):\n",
        "    \"\"\"\n",
        "    Generates a brief, contextual summary for a text chunk based on the entire document.\n",
        "\n",
        "    Args:\n",
        "        document (str): The full content of the original document.\n",
        "        chunk (str): The specific text chunk that needs a contextual summary.\n",
        "\n",
        "    Returns:\n",
        "        str: An AI-generated contextual summary for the chunk.\n",
        "    \"\"\"\n",
        "\n",
        "    # This is the prompt template that instructs the language model.\n",
        "    # It clearly defines the role of the AI, the inputs (the full paper and the specific chunk),\n",
        "    # and the desired output format (a concise context).\n",
        "    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n",
        "                            Your task is to provide brief, relevant context for a chunk of text\n",
        "                            based on the following research paper.\n",
        "\n",
        "                            Here is the research paper:\n",
        "                            <paper>\n",
        "                            {paper}\n",
        "                            </paper>\n",
        "\n",
        "                            Here is the chunk we want to situate within the whole document:\n",
        "                            <chunk>\n",
        "                            {chunk}\n",
        "                            </chunk>\n",
        "\n",
        "                            Provide a concise context (3-4 sentences max) for this chunk,\n",
        "                            considering the following guidelines:\n",
        "\n",
        "                            - Give a short succinct context to situate this chunk within the overall document\n",
        "                            for the purposes of improving search retrieval of the chunk.\n",
        "                            - Answer only with the succinct context and nothing else.\n",
        "                            - Context should be mentioned like 'Focuses on ....'\n",
        "                            do not mention 'this chunk or section focuses on...'\n",
        "\n",
        "                            Context:\n",
        "                         \"\"\"\n",
        "\n",
        "    # Create a prompt object from the string template\n",
        "    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n",
        "\n",
        "    # Define the \"chain\" using LangChain Expression Language (LCEL).\n",
        "    # 1. The prompt_template takes the 'paper' and 'chunk' inputs.\n",
        "    # 2. The formatted prompt is passed to the language model ('chatgpt').\n",
        "    # 3. The model's output is parsed into a simple string by 'StrOutputParser'.\n",
        "    agentic_chunk_chain = (prompt_template\n",
        "                           |\n",
        "                           chatgpt # Your language model instance\n",
        "                           |\n",
        "                           StrOutputParser())\n",
        "\n",
        "    # Execute the chain by providing the actual document and chunk content.\n",
        "    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n",
        "\n",
        "    # Return the generated context string.\n",
        "    return context"
      ],
      "metadata": {
        "id": "mM5akEgv0lHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import uuid # Used for generating unique IDs for each chunk\n",
        "\n",
        "# --- Main Function to Create Contextual Chunks from a File ---\n",
        "\n",
        "def create_contextual_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n",
        "    \"\"\"\n",
        "    Loads a PDF, splits it into chunks, and adds a contextual summary to each chunk.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the PDF file.\n",
        "        chunk_size (int): The maximum number of characters for each chunk.\n",
        "        chunk_overlap (int): The number of characters to overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        list[Document]: A list of LangChain Document objects, where each document's\n",
        "                        page_content is the context + original chunk text.\n",
        "    \"\"\"\n",
        "\n",
        "    print('Loading pages:', file_path)\n",
        "    # Initialize the PDF loader with the file path\n",
        "    loader = PyMuPDFLoader(file_path)\n",
        "    # Load the document into a list of LangChain Document objects, one per page\n",
        "    doc_pages = loader.load()\n",
        "\n",
        "    print('Chunking pages:', file_path)\n",
        "    # Initialize the text splitter with the desired chunk size and overlap\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                              chunk_overlap=chunk_overlap)\n",
        "    # Split the loaded pages into smaller chunks\n",
        "    doc_chunks = splitter.split_documents(doc_pages)\n",
        "\n",
        "    print('Generating contextual chunks:', file_path)\n",
        "    # Reassemble the entire document's text to provide full context for each chunk\n",
        "    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n",
        "\n",
        "    # This list will store the final, enriched chunks\n",
        "    contextual_chunks = []\n",
        "\n",
        "    # Loop through each raw chunk to generate its context\n",
        "    for chunk in doc_chunks:\n",
        "        # Get the text content and metadata from the current chunk\n",
        "        chunk_content = chunk.page_content\n",
        "        chunk_metadata = chunk.metadata\n",
        "\n",
        "        # Create a new, updated metadata dictionary for better organization\n",
        "        chunk_metadata_upd = {\n",
        "            'id': str(uuid.uuid4()),  # Add a unique identifier\n",
        "            'page': chunk_metadata['page'],  # Keep the original page number\n",
        "            'source': chunk_metadata['source'],  # Keep the original file path\n",
        "            'title': chunk_metadata['source'].split('/')[-1] # Extract just the filename as title\n",
        "        }\n",
        "\n",
        "        # Call our helper function to generate the context for this specific chunk\n",
        "        context = generate_chunk_context(original_doc, chunk_content)\n",
        "\n",
        "        # Create a new LangChain Document object\n",
        "        # The content is the generated context, a newline, and then the original chunk's content\n",
        "        # The metadata is our newly created dictionary\n",
        "        contextual_chunks.append(Document(page_content=context + '\\n' + chunk_content,\n",
        "                                          metadata=chunk_metadata_upd))\n",
        "\n",
        "    print('Finished processing:', file_path)\n",
        "    print()\n",
        "\n",
        "    # Return the list of newly created, context-aware chunks\n",
        "    return contextual_chunks"
      ],
      "metadata": {
        "id": "-uxOSfcsxqHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "pdf_files = glob('./rag_docs/*.pdf')\n",
        "pdf_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-VUpdmczt0C",
        "outputId": "699505f0-29f1-4d49-b846-1697f74af83b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./rag_docs/attention_paper.pdf',\n",
              " './rag_docs/cnn_paper.pdf',\n",
              " './rag_docs/vision_transformer.pdf',\n",
              " './rag_docs/resnet_paper.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_docs = []\n",
        "for fp in pdf_files:\n",
        "    paper_docs.extend(create_contextual_chunks(file_path=fp, chunk_size=3500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsicBMPazzlg",
        "outputId": "2eee075f-c03e-4706-f533-d1ae64ba56b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pages: ./rag_docs/attention_paper.pdf\n",
            "Chunking pages: ./rag_docs/attention_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/attention_paper.pdf\n",
            "Finished processing: ./rag_docs/attention_paper.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/cnn_paper.pdf\n",
            "Chunking pages: ./rag_docs/cnn_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/cnn_paper.pdf\n",
            "Finished processing: ./rag_docs/cnn_paper.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/vision_transformer.pdf\n",
            "Chunking pages: ./rag_docs/vision_transformer.pdf\n",
            "Generating contextual chunks: ./rag_docs/vision_transformer.pdf\n",
            "Finished processing: ./rag_docs/vision_transformer.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/resnet_paper.pdf\n",
            "Chunking pages: ./rag_docs/resnet_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/resnet_paper.pdf\n",
            "Finished processing: ./rag_docs/resnet_paper.pdf\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(paper_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOkwPMxp0gFh",
        "outputId": "bcb4b70f-38dc-4c13-cd1c-696a495f9dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVK2i0lR0ieV",
        "outputId": "82cd69cc-fe09-4f88-8c77-5b7a79e28e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'id': '3fc63115-1249-42fa-99a4-3467fe72355b', 'page': 0, 'source': './rag_docs/attention_paper.pdf', 'title': 'attention_paper.pdf'}, page_content='Focuses on the authorship and permissions related to the research paper \"Attention Is All You Need,\" which introduces the Transformer model for sequence transduction tasks. It outlines the contributions of each author and provides details on the paper\\'s presentation at the 31st Conference on Neural Information Processing Systems (NIPS 2017).\\nProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(paper_docs[10].page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "rCtsR5bYzydp",
        "outputId": "28591fcc-0942-4548-a2a5-a5b6c231a8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the performance of the Transformer model in English constituency parsing, comparing its results to various established parsers, both in discriminative and semi-supervised settings. It highlights the model's competitive F1 scores, demonstrating its effectiveness despite a lack of task-specific tuning, and contrasts its performance with that of recurrent neural network-based models.\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser\nTraining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7\nConclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine all document chunks in one list"
      ],
      "metadata": {
        "id": "UyPdlZo2xEly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs_processed)"
      ],
      "metadata": {
        "id": "UbtpR-r50mEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e4d050-2a98-427d-a842-e6f59358adbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1801"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_docs = wiki_docs_processed + paper_docs\n",
        "len(total_docs)"
      ],
      "metadata": {
        "id": "lNQWgq9t0pMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfa5b75-45c7-4d92-8a6d-24629340323d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1880"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index Document Chunks and Embeddings in Vector DB\n",
        "\n",
        "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
      ],
      "metadata": {
        "id": "Daqn6Hglw9Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
        "chroma_db = Chroma.from_documents(documents=total_docs,\n",
        "                                  collection_name='my_context_db',\n",
        "                                  embedding=openai_embed_model,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./my_context_db\")"
      ],
      "metadata": {
        "id": "ZhAQyrFBfvAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ju_zBIj1Zsb"
      },
      "source": [
        "### Load Vector DB from disk\n",
        "\n",
        "This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNvj0dDH1WDg"
      },
      "outputs": [],
      "source": [
        "# load from disk\n",
        "chroma_db = Chroma(persist_directory=\"./my_context_db\",\n",
        "                   collection_name='my_context_db',\n",
        "                   embedding_function=openai_embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFC3uPqYop0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af17825-60cf-4851-afac-acab9b9a10a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7f36027ee180>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "chroma_db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Similarity based Retrieval\n",
        "\n",
        "We use simple cosine similarity here and retrieve the top 5 similar documents based on the user input query"
      ],
      "metadata": {
        "id": "njfZOOVZxj1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "tV1l6HYdxj1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_docs(docs):\n",
        "    for doc in docs:\n",
        "        print('Metadata:', doc.metadata)\n",
        "        print('Content Brief:')\n",
        "        display(Markdown(doc.page_content[:1000]))\n",
        "        print()"
      ],
      "metadata": {
        "id": "nUIJG_bDxj1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is machine learning?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "PIh4xGv2xj1c",
        "outputId": "a1d0feb4-bf43-4585-f9d5-479cc4d43a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'page': 1, 'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '359370', 'title': 'Supervised learning', 'source': 'Wikipedia', 'page': 1}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'Deep learning', 'page': 1, 'id': '663523', 'source': 'Wikipedia'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'page': 1, 'id': '6360', 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'page': 1, 'source': 'Wikipedia', 'title': 'Artificial neural network', 'id': '44742'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the difference between transformers and vision transformers?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "S_PXFMcJxuyO",
        "outputId": "36a83a13-c7a5-4236-a50f-2ee1a5288f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'page': 7, 'id': '3c534858-932f-4bf6-bccd-34154e51cc58', 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on a controlled scaling study of various models, including ResNets and Vision Transformers, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers outperform ResNets in terms of efficiency and scalability, while also discussing the implications for future model scaling efforts.\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/1"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'vision_transformer.pdf', 'id': '561364f3-23f5-428d-b43a-6171c2586694', 'page': 0, 'source': './rag_docs/vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. It highlights the limitations of convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks when pre-trained on large datasets, while requiring fewer computational resources.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '103a7416-3ff5-4abb-91f2-5f3716cd500b', 'title': 'vision_transformer.pdf', 'page': 2, 'source': './rag_docs/vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁ"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'vision_transformer.pdf', 'id': 'c9afddf8-565b-474b-918d-e489eebb1096', 'source': './rag_docs/vision_transformer.pdf', 'page': 1}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the performance of the Vision Transformer (ViT) in comparison to convolutional neural networks (CNNs), highlighting the advantages of large-scale training on datasets ranging from 14M to 300M images. It emphasizes that ViT achieves state-of-the-art results on various image recognition benchmarks when pre-trained on extensive datasets like ImageNet-21k and JFT-300M, despite lacking some inductive biases inherent to CNNs.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset,"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'source': './rag_docs/vision_transformer.pdf', 'id': '6c440742-c527-488f-8356-6bbe2b80cbe7', 'title': 'vision_transformer.pdf', 'page': 7}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the behavior of attention mechanisms in Vision Transformers, highlighting how attention distances vary across layers and the implications of localized attention in hybrid models that incorporate CNNs. It also discusses the relationship between attention distance and network depth, indicating that deeper layers attend to semantically relevant regions for classification.\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁcation (Figure 6).\n4.6\nSELF-SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability bu"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the RAG Pipeline"
      ],
      "metadata": {
        "id": "gQFWv7YUyVII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
        "                Answer the following question using only the following pieces of retrieved context.\n",
        "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
        "                Keep the answer detailed and well formatted based on the information from the context.\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
      ],
      "metadata": {
        "id": "PHOrfGXKyVIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (similarity_retriever\n",
        "                      |\n",
        "                    format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "      |\n",
        "    rag_prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ],
      "metadata": {
        "id": "KmWeCB4yyVIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "query = \"What is machine learning?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "686f7026-0537-4d7a-fdf2-3767bcaa2bbb",
        "id": "xvj_eGIWyVIJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\n\nMachine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Common applications include spam filtering, detecting network intruders or malicious insiders, optical character recognition (OCR), search engines, and computer vision.\n\nWithin machine learning, there are different approaches, such as supervised learning, where a function is inferred from labeled training data. In this case, the system learns to produce correct results based on known outcomes, typically using vectors for training data and results to create a \"classifier.\" Inductive reasoning is often employed to generalize from the training data.\n\nAdditionally, deep learning is a specialized area of machine learning that utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). Deep learning is effective for complex tasks like speech recognition, image understanding, and handwriting recognition, which are challenging for computers but relatively easy for humans. These models are inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains.\n\nIn summary, machine learning enables computers to learn from data and improve their performance over time, making it a crucial component of modern artificial intelligence applications."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a CNN?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "pXtezDlZzadt",
        "outputId": "51231c4e-5fbd-4e06-fbfb-365ff848ff92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A CNN, or Convolutional Neural Network, is a specialized type of Artificial Neural Network (ANN) that is particularly effective for image-driven pattern recognition tasks. CNNs are designed to process data with a grid-like topology, such as images, and they utilize a unique architecture that distinguishes them from traditional ANNs.\n\n### Key Features of CNNs:\n\n1. **Three-Dimensional Neuron Organization**: \n   - The neurons in CNNs are organized in three dimensions, which correspond to the spatial dimensions of the input (height and width) and the depth (which can represent color channels in images).\n\n2. **Layer Types**:\n   - CNNs are composed of three main types of layers:\n     - **Convolutional Layers**: These layers apply convolution operations to the input, allowing the network to learn spatial hierarchies of features. Each neuron in a convolutional layer is connected to a small region of the input, which helps in detecting local patterns.\n     - **Pooling Layers**: These layers reduce the spatial dimensions of the input, helping to decrease the computational load and control overfitting by summarizing the features.\n     - **Fully-Connected Layers**: These layers connect every neuron in one layer to every neuron in the next layer, typically used at the end of the network to produce the final output.\n\n3. **Functionality**:\n   - The architecture of CNNs allows them to effectively learn and extract features from images. The convolutional layers detect features such as edges and textures, while pooling layers help in down-sampling the feature maps, making the network more efficient.\n\n4. **Learning Paradigms**:\n   - CNNs typically utilize supervised learning, where the model is trained on labeled data to minimize classification errors. This is crucial for tasks like image classification, where the goal is to assign a label to an input image.\n\n5. **Applications**:\n   - CNNs are widely used in various applications, including image recognition, object detection, and even in fields like medical image analysis and autonomous driving.\n\nIn summary, CNNs are a powerful architecture within the realm of machine learning, specifically tailored for tasks involving image data, leveraging their unique structure to achieve high performance in pattern recognition."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is a resnet better than a CNN?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "Fo92-ZmIELPF",
        "outputId": "32ca7fee-f8b4-4873-a6bd-51ec3f97866c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A ResNet (Residual Network) is considered better than a traditional CNN (Convolutional Neural Network) for several reasons, primarily due to its architectural innovations that address common challenges faced in training deep networks. Here are the key advantages of ResNets over standard CNNs:\n\n1. **Residual Learning Framework**: \n   - ResNets introduce the concept of residual learning, where the network learns to predict the residual (the difference) between the desired output and the input. This is mathematically represented as \\( F(x) + x \\), where \\( F(x) \\) is the residual mapping. This approach simplifies the learning process, making it easier for the network to optimize deeper architectures.\n\n2. **Shortcut Connections**:\n   - The architecture of ResNets includes shortcut connections that skip one or more layers. These connections allow gradients to flow more easily during backpropagation, mitigating issues like vanishing gradients that often occur in very deep networks. This results in more effective training of deeper models.\n\n3. **Degradation Problem**:\n   - Traditional CNNs often suffer from the degradation problem, where adding more layers leads to higher training error. In contrast, ResNets can maintain or even improve performance as depth increases. For instance, a 34-layer ResNet outperforms an 18-layer ResNet, which is not the case for plain networks where deeper models can lead to worse performance.\n\n4. **Higher Accuracy with Increased Depth**:\n   - ResNets can achieve significantly better accuracy with increased depth without the degradation issues. For example, the 152-layer ResNet has shown to outperform shallower models and even other architectures like VGG, achieving lower top-5 validation errors in competitions.\n\n5. **Computational Efficiency**:\n   - Despite being deeper, ResNets can have lower computational complexity (measured in FLOPs) compared to other architectures like VGG. For instance, a 152-layer ResNet has lower complexity than VGG-16, allowing for more efficient training and inference.\n\n6. **Generalization Performance**:\n   - ResNets have demonstrated superior generalization performance across various tasks, including object detection and image classification, as evidenced by their success in competitions like ILSVRC and COCO.\n\nIn summary, the architectural innovations of ResNets, particularly the use of residual learning and shortcut connections, enable them to train deeper networks effectively, avoid degradation problems, and achieve higher accuracy compared to traditional CNNs."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is NLP and its relation to linguistics?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "J5IQoBc0zlAr",
        "outputId": "59ba83a4-abd3-4d79-b33e-bbecde163e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Natural Language Processing (NLP) is a field within Artificial Intelligence that focuses on enabling computers to automatically understand and generate human languages. The term \"Natural Language\" specifically refers to human languages, distinguishing them from programming languages. The overarching goal of NLP is to facilitate seamless interaction between humans and machines through language.\n\nNLP is closely related to linguistics, as it draws upon linguistic principles to enhance the understanding and processing of human language. Linguistics provides the foundational theories and frameworks that inform how language is structured, used, and understood, which are essential for developing effective NLP systems. By leveraging insights from linguistics, NLP aims to improve the accuracy and efficiency of language-related tasks, such as speech recognition, language translation, and text analysis."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between AI, ML and DL?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "AzeZuG1hzvGy",
        "outputId": "40912bf5-735c-4e9a-b8af-938b882f08be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The difference between AI, ML, and DL can be summarized as follows:\n\n### Artificial Intelligence (AI)\n- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications aimed at making machines \"smart.\"\n- **Functionality**: AI systems can interpret external data, learn from it, and use those learnings to achieve specific goals through flexible adaptation. The term has evolved, and tasks once considered AI, like optical character recognition, are now seen as routine technologies.\n- **Origin**: The term \"Artificial Intelligence\" was coined by John McCarthy in 1955.\n\n### Machine Learning (ML)\n- **Definition**: ML is a subfield of AI that focuses on the study and construction of algorithms that allow computers to learn from and make predictions based on data without being explicitly programmed.\n- **Functionality**: ML algorithms build models from sample inputs and can make predictions or decisions based on data. It is particularly useful in scenarios where designing explicit algorithms is impractical.\n- **Examples**: Applications of ML include spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.\n\n### Deep Learning (DL)\n- **Definition**: DL is a specialized subset of ML that primarily uses neural networks with multiple layers (deep neural networks) to analyze various forms of data.\n- **Functionality**: In DL, learning sessions can be unsupervised, semi-supervised, or supervised. The architecture typically includes at least one hidden layer between the input and output layers, allowing the model to process information in a more abstract manner as it passes through the layers.\n- **Applications**: DL is particularly effective for complex tasks such as speech recognition, image classification, and understanding handwriting, which are challenging for traditional algorithms.\n\nIn summary, AI is the overarching field that includes both ML and DL, with ML being a method within AI that enables learning from data, and DL being a more advanced method of ML that utilizes deep neural networks for complex data processing."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between transformers and vision transformers?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "cWihDiL3zPzY",
        "outputId": "d46da897-f48c-4df2-91b7-30c93c9a56e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The difference between transformers and vision transformers (ViTs) primarily lies in their application and the way they process input data.\n\n### Transformers\n- **Application**: Transformers were originally designed for natural language processing (NLP) tasks. They excel in handling sequential data, where the input is typically a sequence of tokens (words).\n- **Input Processing**: In NLP, transformers take a sequence of word embeddings as input. Each token in the sequence is treated independently, and the model uses self-attention mechanisms to capture relationships between tokens across the entire sequence.\n- **Architecture**: The standard transformer architecture consists of layers of multi-headed self-attention and feedforward neural networks, with positional encodings added to maintain the order of tokens.\n\n### Vision Transformers (ViTs)\n- **Application**: Vision transformers adapt the transformer architecture for image classification tasks. They treat image patches as tokens, allowing the model to process visual data similarly to how it processes text.\n- **Input Processing**: In ViTs, an image is divided into fixed-size patches, which are then flattened and linearly embedded into a sequence of vectors. This sequence is fed into the transformer model, where each patch is treated like a token in NLP.\n- **Architecture**: ViTs maintain the core transformer structure but include additional components like position embeddings to retain spatial information about the patches. The model uses self-attention to integrate information across the entire image, enabling it to capture global context effectively.\n\n### Key Differences\n1. **Data Type**: Transformers are designed for sequential data (text), while ViTs are tailored for 2D image data.\n2. **Input Representation**: Transformers use word tokens, whereas ViTs use image patches as tokens.\n3. **Inductive Bias**: Traditional CNNs, which are often used in computer vision, have strong inductive biases such as locality and translation equivariance. In contrast, ViTs have less image-specific inductive bias, relying more on the data to learn spatial relationships.\n4. **Performance**: ViTs can achieve competitive performance on image classification tasks, especially when pre-trained on large datasets, often requiring fewer computational resources compared to traditional CNNs.\n\nIn summary, while both transformers and vision transformers share a foundational architecture, they differ significantly in their input processing, application domains, and the types of data they are optimized to handle."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is self-attention important in transformers?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "k1lqzejlEvsj",
        "outputId": "f1dbaec1-baaa-45a4-8f54-ec936bee073a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Self-attention is a crucial mechanism in transformers that allows the model to capture dependencies across different positions in a sequence, regardless of their distance. This capability is particularly important for several reasons:\n\n1. **Global Dependency Modeling**: Self-attention enables the transformer to model relationships between all parts of the input sequence simultaneously. Unlike recurrent neural networks (RNNs), which process sequences in a linear fashion and can struggle with long-range dependencies, self-attention allows the transformer to attend to any part of the sequence directly. This is essential for tasks that require understanding context across long distances, such as language translation and comprehension.\n\n2. **Parallelization**: The architecture of transformers, which relies entirely on self-attention rather than recurrence, allows for significant parallelization during training. This leads to faster training times and the ability to handle longer sequences more efficiently. The transformer can process all input positions at once, making it more scalable compared to traditional RNNs.\n\n3. **Attention Mechanism**: Self-attention computes a representation of a sequence by relating different positions within that same sequence. This mechanism is effective in various tasks, including reading comprehension and summarization, as it allows the model to focus on relevant parts of the input when generating outputs.\n\n4. **Layer-wise Attention Behavior**: In the context of Vision Transformers, self-attention shows varying attention distances across layers. Lower layers tend to have localized attention, focusing on small regions of the input, while deeper layers attend to more semantically relevant regions for classification. This hierarchical attention behavior enhances the model's ability to integrate information effectively.\n\n5. **Multi-Head Attention**: The use of multi-head attention in transformers allows the model to capture different types of relationships and dependencies simultaneously. Each attention head can focus on different aspects of the input, enriching the representation learned by the model.\n\nIn summary, self-attention is vital in transformers as it facilitates the modeling of complex dependencies, enhances computational efficiency through parallelization, and allows for a flexible and powerful representation of input sequences."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How does a resnet work?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "7Zf_BjmlFBcb",
        "outputId": "d6e8645d-9948-431d-b310-60a798ddbdb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A ResNet, or Residual Network, is a type of deep neural network architecture designed to address the degradation problem that occurs when training very deep networks. The key innovation of ResNets is the introduction of residual learning through the use of shortcut connections.\n\n### How ResNets Work:\n\n1. **Residual Mapping**:\n   - Instead of learning the desired underlying mapping \\( H(x) \\) directly, ResNets learn a residual mapping \\( F(x) = H(x) - x \\). This means that the network is trained to predict the difference between the desired output and the input, rather than the output itself.\n   - The original mapping can then be expressed as \\( H(x) = F(x) + x \\). This formulation allows the network to focus on learning the residuals, which is often easier than learning the full mapping directly.\n\n2. **Shortcut Connections**:\n   - ResNets utilize shortcut connections that skip one or more layers. These connections perform identity mapping, meaning they add the input \\( x \\) directly to the output of the stacked layers. This addition does not introduce any extra parameters or computational complexity.\n   - When the dimensions of the input and output differ, the shortcut can either use zero-padding (option A) or a projection shortcut using 1x1 convolutions (option B) to match dimensions.\n\n3. **Optimization Benefits**:\n   - The introduction of residual connections helps mitigate issues such as vanishing gradients, which can hinder the training of deep networks. By allowing gradients to flow more easily through the network, ResNets can be trained effectively even with hundreds or thousands of layers.\n   - Empirical results show that deeper ResNets achieve better performance and lower training errors compared to traditional plain networks of similar depth. For instance, a 34-layer ResNet outperforms an 18-layer ResNet, demonstrating that deeper architectures can be beneficial when using residual learning.\n\n4. **Training Procedure**:\n   - ResNets are typically trained using stochastic gradient descent (SGD) with techniques such as batch normalization and data augmentation to improve convergence and generalization.\n   - The learning rate is often adjusted during training to optimize convergence, starting from a higher value and decreasing it as training progresses.\n\n### Conclusion:\nOverall, ResNets represent a significant advancement in deep learning, allowing for the construction of very deep networks that are easier to optimize and achieve superior performance on tasks such as image classification. The residual learning framework has proven to be effective across various datasets, including CIFAR-10 and ImageNet, leading to state-of-the-art results in numerous competitions."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is LangGraph?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "UbaUpVXKz8IK",
        "outputId": "d0fad3ad-1c02-4147-aee1-5f9ab9b2f12d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is an Agentic AI System?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "FmxNo6B50AdP",
        "outputId": "dad20f3e-6e16-4467-bdcf-8a60ec08ad7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is LangChain?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "Ttz53mEy0J_D",
        "outputId": "f5b48274-27cc-43f5-bf74-290324f5d75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a RAG System with Sources"
      ],
      "metadata": {
        "id": "-HkUAnWFH8LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
        "                Answer the following question using only the following pieces of retrieved context.\n",
        "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
        "                Keep the answer detailed and well formatted based on the information from the context.\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
      ],
      "metadata": {
        "id": "cd9CWH6hH8LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary components from LangChain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from operator import itemgetter # A standard Python function to get an item from a dictionary\n",
        "\n",
        "# --- Initialization ---\n",
        "# Initialize the language model instance. We'll use this for generation.\n",
        "# \"gpt-4o-mini\" is a fast and capable model. temperature=0 makes its output more deterministic.\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# NOTE: The following variables are assumed to be defined elsewhere in your code:\n",
        "# - similarity_retriever: A LangChain retriever object that fetches documents based on a query.\n",
        "# - rag_prompt_template: A ChatPromptTemplate that takes \"context\" and \"question\" as input.\n",
        "\n",
        "\n",
        "# --- Helper Function ---\n",
        "def format_docs(docs):\n",
        "    \"\"\"A helper function to concatenate the page_content of multiple documents into a single string.\"\"\"\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# --- Chain 1: The Core RAG Generation Chain ---\n",
        "# This chain is responsible for generating the final answer once it has the context and question.\n",
        "src_rag_response_chain = (\n",
        "    # This dictionary defines the inputs for the prompt template.\n",
        "    {\n",
        "        # The \"context\" key's value is processed in two steps:\n",
        "        # 1. itemgetter('context'): Extracts the list of documents from the input dictionary.\n",
        "        # 2. RunnableLambda(format_docs): Applies our helper function to format the documents into a single string.\n",
        "        \"context\": (itemgetter('context') | RunnableLambda(format_docs)),\n",
        "\n",
        "        # The \"question\" key's value is simply extracted from the input dictionary.\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    |\n",
        "    # The dictionary with formatted context and question is passed to the prompt template.\n",
        "    rag_prompt_template\n",
        "    |\n",
        "    # The formatted prompt is passed to the language model.\n",
        "    chatgpt\n",
        "    |\n",
        "    # The output from the model is parsed into a clean string.\n",
        "    StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# --- Chain 2: The Full RAG Pipeline that includes Sources ---\n",
        "# This is the main chain that orchestrates the entire process, from question to final output.\n",
        "rag_chain_w_sources = (\n",
        "    # This initial step performs retrieval. The input to this whole chain is the user's question.\n",
        "    {\n",
        "        # The retriever is called with the user's question to fetch relevant documents.\n",
        "        # The result is assigned to the \"context\" key.\n",
        "        \"context\": similarity_retriever,\n",
        "\n",
        "        # RunnablePassthrough() simply passes the original user question through to the \"question\" key.\n",
        "        # The output of this dictionary is: {\"context\": [docs], \"question\": \"user's question\"}\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    |\n",
        "    # RunnablePassthrough.assign() is a key component. It takes the dictionary from the previous step\n",
        "    # and adds a new key to it. Here, we are adding a key named \"response\".\n",
        "    # The value of \"response\" is generated by running the `src_rag_response_chain` with the\n",
        "    # context and question from the previous step.\n",
        "    RunnablePassthrough.assign(response=src_rag_response_chain)\n",
        ")\n",
        "\n",
        "# Final Output Structure when invoking `rag_chain_w_sources`:\n",
        "# {\n",
        "#   'context': [Document(page_content='...'), ...],  <- The source documents\n",
        "#   'question': 'What is the capital of France?',     <- The original question\n",
        "#   'response': 'The capital of France is Paris.'     <- The LLM's answer\n",
        "# }"
      ],
      "metadata": {
        "id": "m6wFtKQqXNHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is machine learning?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYfU-VdjYf1u",
        "outputId": "fb89ce81-59b4-4f74-a94b-c9922b339673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(id='23437141-e83c-4a58-b26d-df3ef3e2d68e', metadata={'source': 'Wikipedia', 'id': '564928', 'title': 'Machine learning', 'page': 1}, page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'),\n",
              "  Document(id='e819102a-7ef6-411a-a919-210a86db4cb8', metadata={'source': 'Wikipedia', 'id': '359370', 'title': 'Supervised learning', 'page': 1}, page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.'),\n",
              "  Document(id='09e90054-ded4-4823-b01b-999e4a96b03f', metadata={'page': 1, 'title': 'Deep learning', 'source': 'Wikipedia', 'id': '663523'}, page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.'),\n",
              "  Document(id='44039d8c-b2ef-4d08-bad1-ba0adfc8ac6a', metadata={'id': '6360', 'title': 'Artificial intelligence', 'page': 1, 'source': 'Wikipedia'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n",
              "  Document(id='ffc088e9-efaa-49e4-bd88-3da309c201b9', metadata={'page': 1, 'id': '44742', 'title': 'Artificial neural network', 'source': 'Wikipedia'}, page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.')],\n",
              " 'question': 'What is machine learning?',\n",
              " 'response': 'Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\\n\\nMachine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Common applications include spam filtering, detecting network intruders or malicious insiders, optical character recognition (OCR), search engines, and computer vision.\\n\\nWithin machine learning, there are different types of learning approaches, such as supervised learning, where a function is inferred from labeled training data. In this case, the system learns to produce correct results based on known outcomes, typically using vectors for training data and results to create a \"classifier.\" Inductive reasoning is often employed to generalize from the training data.\\n\\nAdditionally, deep learning is a specialized area of machine learning that utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). Deep learning is effective for complex tasks like speech recognition, image understanding, and handwriting recognition, which are challenging for computers but relatively easy for humans. These models are inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains.'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_results(result_obj):\n",
        "    print('Query:')\n",
        "    display(Markdown(result_obj['question']))\n",
        "    print()\n",
        "    print('Response:')\n",
        "    display(Markdown(result_obj['response']))\n",
        "    print('='*50)\n",
        "    print('Sources:')\n",
        "    for source in result_obj['context']:\n",
        "        print('Metadata:', source.metadata)\n",
        "        print('Content Brief:')\n",
        "        display(Markdown(source.page_content))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "2wWdkU4oa1BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is machine learning?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8NTfGcChbFuj",
        "outputId": "84741a15-b09f-4a45-cb87-c36ba43307f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "What is machine learning?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\n\nMachine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Common applications include spam filtering, detecting network intruders or malicious insiders, optical character recognition (OCR), search engines, and computer vision.\n\nWithin machine learning, there are different approaches, such as supervised learning, where a function is inferred from labeled training data. In this case, the system learns to produce correct results based on known outcomes, typically using vectors for training data and results to create a \"classifier.\" Inductive reasoning is often employed to generalize from the training data.\n\nAdditionally, deep learning is a specialized area of machine learning that utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). These networks process information in increasingly abstract ways as more layers are added, making them effective for complex tasks like speech and image recognition. Deep learning models are inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains.\n\nIn summary, machine learning encompasses a range of techniques and applications that enable computers to learn from data, adapt, and make informed predictions or decisions without explicit programming."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Sources:\n",
            "Metadata: {'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning', 'page': 1}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '359370', 'title': 'Supervised learning', 'page': 1, 'source': 'Wikipedia'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'source': 'Wikipedia', 'title': 'Deep learning', 'id': '663523', 'page': 1}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'Artificial intelligence', 'page': 1, 'id': '6360', 'source': 'Wikipedia'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'Artificial neural network', 'id': '44742', 'page': 1, 'source': 'Wikipedia'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between AI, ML and DL?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jhWbcmodbYfy",
        "outputId": "09a7dcdf-0a71-4bb2-9255-c940cb512221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "What is the difference between AI, ML and DL?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The difference between AI, ML, and DL can be summarized as follows:\n\n### Artificial Intelligence (AI)\n- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications that enable machines to perform tasks that typically require human intelligence.\n- **Scope**: AI is a field of study aimed at creating systems that can interpret external data, learn from it, and adapt to achieve specific goals. It includes various subfields, including machine learning and deep learning.\n- **Examples**: AI applications can range from simple rule-based systems to complex algorithms that can learn and adapt over time.\n\n### Machine Learning (ML)\n- **Definition**: ML is a subfield of AI that focuses on the development of algorithms that allow computers to learn from and make predictions based on data without being explicitly programmed.\n- **Functionality**: ML algorithms build models from sample inputs and can make decisions or predictions based on new data. It is particularly useful in scenarios where traditional programming is impractical.\n- **Examples**: Applications of ML include spam filtering, network intrusion detection, optical character recognition (OCR), and computer vision.\n\n### Deep Learning (DL)\n- **Definition**: DL is a specialized subset of machine learning that utilizes neural networks with multiple layers (known as deep neural networks) to analyze various forms of data.\n- **Architecture**: Deep learning models often have at least one hidden layer between the input and output layers, allowing them to process information in a more abstract manner. This architecture is inspired by the biological nervous system.\n- **Applications**: DL is particularly effective for complex tasks such as speech recognition, image classification, and natural language processing, where traditional ML methods may struggle.\n\nIn summary, AI is the overarching field that includes both ML and DL, with ML being a method of achieving AI through data-driven learning, and DL being a more advanced technique within ML that uses deep neural networks for complex data analysis."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Sources:\n",
            "Metadata: {'source': 'Wikipedia', 'title': 'Deep learning', 'page': 1, 'id': '663523'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'Machine learning', 'id': '564928', 'page': 1, 'source': 'Wikipedia'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '6360', 'source': 'Wikipedia', 'page': 1, 'title': 'Artificial intelligence'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'source': './rag_docs/cnn_paper.pdf', 'title': 'cnn_paper.pdf', 'page': 3, 'id': '895d04c8-663a-445f-a764-9026142ba3ad'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architectural differences of Convolutional Neural Networks (CNNs) compared to traditional Artificial Neural Networks (ANNs), emphasizing the three-dimensional organization of neurons and the specific types of layers that comprise CNNs, including convolutional, pooling, and fully-connected layers. It also outlines the functionality of these layers in processing image data for classification tasks.\n4\nKeiron O’Shea et al.\nOne of the key differences is that the neurons that the layers within the CNN\nare comprised of neurons organised into three dimensions, the spatial dimen-\nsionality of the input (height and the width) and the depth. The depth does not\nrefer to the total number of layers within the ANN, but the third dimension of a\nactivation volume. Unlike standard ANNS, the neurons within any given layer\nwill only connect to a small region of the layer preceding it.\nIn practice this would mean that for the example given earlier, the input ’vol-\nume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), lead-\ning to a ﬁnal output layer comprised of a dimensionality of 1 × 1 × n (where\nn represents the possible number of classes) as we would have condensed the\nfull input dimensionality into a smaller volume of class scores ﬁled across the\ndepth dimension.\n2.1\nOverall architecture\nCNNs are comprised of three types of layers. These are convolutional layers,\npooling layers and fully-connected layers. When these layers are stacked, a\nCNN architecture has been formed. A simpliﬁed CNN architecture for MNIST\nclassiﬁcation is illustrated in Figure 2.\ninput\n0\n9\nconvolution\n w/ReLu\npooling\noutput\nfully-connected\nw/ ReLu\nfully-connected\n...\nFig. 2: An simple CNN architecture, comprised of just ﬁve layers\nThe basic functionality of the example CNN above can be broken down into\nfour key areas.\n1. As found in other forms of ANN, the input layer will hold the pixel values\nof the image.\n2. The convolutional layer will determine the output of neurons of which are\nconnected to local regions of the input through the calculation of the scalar\nproduct between their weights and the region connected to the input vol-\nume. The rectiﬁed linear unit (commonly shortened to ReLu) aims to apply"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'source': 'Wikipedia', 'id': '669662', 'page': 1, 'title': 'Loop AI Labs'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Loop AI Labs is an AI and cognitive computing company that focuses on language understanding technology. The company was founded in San Francisco in 2012 by Italian entrepreneur Gianmauro Calafiore, who sold his company Gsmbox to in 2004 and then relocated from Italy to San Francisco. Wanting to start an artificial intelligence company, he recruited two veterans of the project, the largest government-funded AI project in history, who had worked on the project at and Stanford University's . The original company name, \"Soshoma\", was changed to Loop AI Labs in 2015 after the company decided to change its focus from consumer-oriented to enterprise. Loop AI Labs is headquartered in San Francisco, California, with offices in New York, Milan, and Singapore. The company is privately funded. On May 4, 2017, Loop AI Labs entered into a deal with , a leading European provider of mobile messaging and solutions, to bring their cognitive computing technology to LINK's business clients, which cover 234 million people across Europe."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between transformers and vision transformers?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iKggz7qfYkh1",
        "outputId": "b2050f6f-a40f-4331-8be5-1096f077875e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "What is the difference between transformers and vision transformers?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The difference between transformers and vision transformers (ViTs) primarily lies in their application and the way they process input data.\n\n1. **Input Data Representation**:\n   - **Transformers**: Originally designed for natural language processing (NLP), transformers operate on sequences of tokens, where each token typically represents a word or sub-word in a text. The input is a 1D sequence of these token embeddings.\n   - **Vision Transformers (ViTs)**: ViTs adapt the transformer architecture for image classification tasks by treating image patches as tokens. An image is divided into fixed-size patches, which are then flattened and linearly embedded into a sequence of vectors. This sequence is fed into the transformer, similar to how words are processed in NLP.\n\n2. **Architecture**:\n   - **Transformers**: The standard transformer architecture consists of layers of multi-headed self-attention and feedforward neural networks, designed to capture relationships and dependencies in sequential data.\n   - **Vision Transformers**: ViTs maintain the core transformer architecture but modify the input to accommodate 2D image data. They utilize position embeddings to retain spatial information about the patches, allowing the model to learn the relationships between different parts of the image.\n\n3. **Inductive Bias**:\n   - **Transformers**: In NLP, transformers inherently leverage the sequential nature of text, which provides a strong inductive bias for language tasks.\n   - **Vision Transformers**: ViTs have much less image-specific inductive bias compared to convolutional neural networks (CNNs). While CNNs incorporate locality and translation equivariance into their architecture, ViTs rely on the self-attention mechanism to integrate information across the entire image, which can be advantageous when trained on large datasets.\n\n4. **Performance and Efficiency**:\n   - **Transformers**: In NLP, transformers have become the standard due to their scalability and performance on large text corpora.\n   - **Vision Transformers**: ViTs have shown competitive performance in image classification tasks, especially when pre-trained on large datasets. They can achieve state-of-the-art results while requiring fewer computational resources compared to traditional CNNs, particularly when scaled appropriately.\n\nIn summary, while both transformers and vision transformers share a common architectural foundation, they differ significantly in their input data handling, inductive biases, and application domains, with ViTs specifically tailored for image processing tasks."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Sources:\n",
            "Metadata: {'source': './rag_docs/vision_transformer.pdf', 'id': '561364f3-23f5-428d-b43a-6171c2586694', 'title': 'vision_transformer.pdf', 'page': 0}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. It highlights the limitations of convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks when pre-trained on large datasets, while requiring fewer computational resources.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train.1\n1\nINTRODUCTION\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance.\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020).\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classiﬁcation in supervised fashion.\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\n1Fine-tuning\ncode\nand\npre-trained\nmodels\nare\navailable\nat\nhttps://github.com/\ngoogle-research/vision_transformer\n1\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'vision_transformer.pdf', 'page': 7, 'id': '3c534858-932f-4bf6-bccd-34154e51cc58', 'source': './rag_docs/vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on a controlled scaling study of various models, including ResNets and Vision Transformers, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers outperform ResNets in terms of efficiency and scalability, while also discussing the implications for future model scaling efforts.\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone).\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\n4.5\nINSPECTING VISION TRANSFORMER\nInput\nAttention\nFigure 6: Representative ex-\namples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails.\nTo begin to understand how the Vision Transformer processes im-\nage data, we analyze its internal representations. The ﬁrst layer of\nthe Vision Transformer linearly projects the ﬂattened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding ﬁlters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the ﬁne structure within each patch.\nAfter the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position em-\nbeddings, i.e. closer patches tend to have more similar position em-\nbeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology ex-\nplains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4).\nSelf-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speciﬁcally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf', 'page': 2, 'id': '103a7416-3ff5-4abb-91f2-5f3716cd500b'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3\nMETHOD\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\ntheir efﬁcient implementations – can be used almost out of the box.\n3.1\nVISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder (z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\ntached to z0\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at ﬁne-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n3"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'page': 1, 'title': 'vision_transformer.pdf', 'id': 'c9afddf8-565b-474b-918d-e489eebb1096', 'source': './rag_docs/vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the performance of the Vision Transformer (ViT) in comparison to convolutional neural networks (CNNs), highlighting the advantages of large-scale training on datasets ranging from 14M to 300M images. It emphasizes that ViT achieves state-of-the-art results on various image recognition benchmarks when pre-trained on extensive datasets like ImageNet-21k and JFT-300M, despite lacking some inductive biases inherent to CNNs.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks.\n2\nRELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\nNaive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nefﬁciently on hardware accelerators.\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well.\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': 'e45099b3-f31d-4258-bbaf-3e6a4a113a4c', 'page': 3, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and operational details of the Vision Transformer (ViT), including the structure of the multi-layer perceptron (MLP) and the handling of inductive biases compared to convolutional neural networks (CNNs). It also discusses the hybrid architecture that combines CNN feature maps with ViT and outlines the fine-tuning process for downstream tasks, emphasizing the importance of resolution adjustments and the implications for model performance.\nPublished as a conference paper at ICLR 2021\nThe MLP contains two layers with a GELU non-linearity.\nz0 = [xclass; x1\npE; x2\npE; · · · ; xN\np E] + Epos,\nE ∈R(P 2·C)×D, Epos ∈R(N+1)×D\n(1)\nz′\nℓ= MSA(LN(zℓ−1)) + zℓ−1,\nℓ= 1 . . . L\n(2)\nzℓ= MLP(LN(z′\nℓ)) + z′\nℓ,\nℓ= 1 . . . L\n(3)\ny = LN(z0\nL)\n(4)\nInductive bias.\nWe note that Vision Transformer has much less image-speciﬁc inductive bias than\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\nat ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\nscribed below). Other than that, the position embeddings at initialization time carry no information\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\nfrom scratch.\nHybrid Architecture.\nAs an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\nﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nThe classiﬁcation input embedding and position embeddings are added as described above.\n3.2\nFINE-TUNING AND HIGHER RESOLUTION\nTypically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For\nthis, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward\nlayer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\n2D interpolation of the pre-trained position embeddings, according to their location in the original\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\n4\nEXPERIMENTS\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\nthat self-supervised ViT holds promise for the future.\n4.1\nSETUP\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is an Agentic AI System?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "fCCvcwzEbqQq",
        "outputId": "7ddf7055-aa2d-40a4-f0ac-7a5dce395c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "What is an Agentic AI System?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Sources:\n",
            "Metadata: {'id': '6360', 'source': 'Wikipedia', 'title': 'Artificial intelligence', 'page': 1}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'source': 'Wikipedia', 'page': 1, 'id': '674015', 'title': 'A.I. Artificial Intelligence'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A.I. Artificial Intelligence, or A.I., is a 2001 American science fiction drama movie directed by Steven Spielberg. The screenplay was by Spielberg based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. The movie was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt. It is set in a futuristic post-climate change society. \"A.I.\" tells the story of David (Osment), a childlike android uniquely programmed with the ability to love."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'Swarm intelligence', 'source': 'Wikipedia', 'id': '112634', 'page': 1}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Swarm Intelligence is a field of Computer science. It is a form of Artificial intelligence. Some animals, mostly insects like ants, or bees form large colonies. These colonies are made of many animals that communicate with each other. Each animal is relatively simple, but by working together with other animals it is able to solve complex tasks. Swarm intelligence wants to obtain similar behaviour than that observed with these animals. Instead of the animals, so called \"agents\" are used."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'Shakey the robot', 'page': 1, 'source': 'Wikipedia', 'id': '692745'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Shakey the Robot was the first general purpose mobile AI robot. The project combined research in robotics, computer vision, and natural language processing. Because of this, it was the first project that melded logical reasoning and physical action. Shakey was developed at the Artificial Intelligence Center of Stanford Research Institute (now called SRI International) by Nils John Nilsson from 1966 to 1972."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'title': 'Robot lawyer', 'id': '564218', 'source': 'Wikipedia', 'page': 1}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A robot lawyer is an artificial intelligence (AI) computer program. It is designed to ask the same questions as a real lawyer about certain legal issues. Robot lawyers are being used in many countries around the world including the United States, the United Kingdom, and Holland."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a RAG System with Source Citations Agentic Pipeline"
      ],
      "metadata": {
        "id": "25Si_mSAc9HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
        "                Answer the following question using only the following pieces of retrieved context.\n",
        "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
        "                Keep the answer detailed and well formatted based on the information from the context.\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n",
        "rag_prompt_template.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3cKwTUIc9HZ",
        "outputId": "ec8311c5-78b6-4083-a591-2b95830dcc84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "You are an assistant who is an expert in question-answering tasks.\n",
            "                Answer the following question using only the following pieces of retrieved context.\n",
            "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
            "                Keep the answer detailed and well formatted based on the information from the context.\n",
            "\n",
            "                Question:\n",
            "                \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
            "\n",
            "                Context:\n",
            "                \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
            "\n",
            "                Answer:\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "citations_prompt = \"\"\"You are an assistant who is an expert in analyzing answers to questions\n",
        "                      and finding out referenced citations from context articles.\n",
        "\n",
        "                      Given the following question, context and generated answer,\n",
        "                      analyze the generated answer and quote citations from context articles\n",
        "                      that can be used to justify the generated answer.\n",
        "\n",
        "                      Question:\n",
        "                      {question}\n",
        "\n",
        "                      Context Articles:\n",
        "                      {context}\n",
        "\n",
        "                      Answer:\n",
        "                      {answer}\n",
        "                  \"\"\"\n",
        "\n",
        "cite_prompt_template = ChatPromptTemplate.from_template(citations_prompt)\n",
        "cite_prompt_template.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fbhOllRprRx",
        "outputId": "7fff4ca0-3e1c-4437-ff18-89992a406842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "You are an assistant who is an expert in analyzing answers to questions\n",
            "                      and finding out referenced citations from context articles.\n",
            "\n",
            "                      Given the following question, context and generated answer,\n",
            "                      analyze the generated answer and quote citations from context articles\n",
            "                      that can be used to justify the generated answer.\n",
            "\n",
            "                      Question:\n",
            "                      \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
            "\n",
            "                      Context Articles:\n",
            "                      \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
            "\n",
            "                      Answer:\n",
            "                      \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
            "                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class Citation(BaseModel):\n",
        "    id: str = Field(description=\"\"\"The string ID of a SPECIFIC context article\n",
        "                                   which justifies the answer.\"\"\")\n",
        "    source: str = Field(description=\"\"\"The source of the SPECIFIC context article\n",
        "                                       which justifies the answer.\"\"\")\n",
        "    title: str = Field(description=\"\"\"The title of the SPECIFIC context article\n",
        "                                      which justifies the answer.\"\"\")\n",
        "    page: int = Field(description=\"\"\"The page number of the SPECIFIC context article\n",
        "                                     which justifies the answer.\"\"\")\n",
        "    quotes: str = Field(description=\"\"\"The VERBATIM sentences from the SPECIFIC context article\n",
        "                                      that are used to generate the answer.\n",
        "                                      Should be exact sentences from context article without missing words.\"\"\")\n",
        "\n",
        "\n",
        "class QuotedCitations(BaseModel):\n",
        "    \"\"\"Quote citations from given context articles\n",
        "       that can be used to justify the generated answer. Can be multiple articles.\"\"\"\n",
        "    citations: List[Citation] = Field(description=\"\"\"Citations (can be multiple) from the given\n",
        "                                                     context articles that justify the answer.\"\"\")"
      ],
      "metadata": {
        "id": "n-ehwnM4dyGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "# --- Model Initialization ---\n",
        "\n",
        "# Standard language model for generating plain text answers.\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# A specialized version of the model that is forced to output JSON matching the 'QuotedCitations' schema.\n",
        "# This is the key to getting reliable, structured citation data.\n",
        "structured_chatgpt = chatgpt.with_structured_output(QuotedCitations)\n",
        "\n",
        "# NOTE: These variables are assumed to be defined elsewhere:\n",
        "# - similarity_retriever: A retriever that fetches relevant documents.\n",
        "# - rag_prompt_template: A prompt for generating the initial answer.\n",
        "# - cite_prompt_template: A prompt specifically for generating citations based on an answer.\n",
        "\n",
        "\n",
        "# --- Helper Function ---\n",
        "\n",
        "def format_docs_with_metadata(docs: List[Document]) -> str:\n",
        "    \"\"\"\n",
        "    Formats documents to include their metadata explicitly in the context string.\n",
        "    This helps the LLM to easily access information needed for citations.\n",
        "    \"\"\"\n",
        "    formatted_docs = [\n",
        "        f\"\"\"Context Article ID: {doc.metadata['id']}\n",
        "            Context Article Source: {doc.metadata['source']}\n",
        "            Context Article Title: {doc.metadata['title']}\n",
        "            Context Article Page: {doc.metadata['page']}\n",
        "            Context Article Details: {doc.page_content}\n",
        "         \"\"\"\n",
        "        for i, doc in enumerate(docs)\n",
        "    ]\n",
        "    # Join all formatted doc strings into a single block of text.\n",
        "    return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "\n",
        "# --- Chain 1: Generates the initial text answer ---\n",
        "\n",
        "rag_response_chain = (\n",
        "    {\n",
        "        # Format the retrieved documents using our detailed helper function.\n",
        "        \"context\": (itemgetter('context') | RunnableLambda(format_docs_with_metadata)),\n",
        "        # Pass the original question through.\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    |\n",
        "    # Use the standard RAG prompt.\n",
        "    rag_prompt_template\n",
        "    |\n",
        "    # Use the standard chat model to generate a text answer.\n",
        "    chatgpt\n",
        "    |\n",
        "    # Parse the output into a string.\n",
        "    StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# --- Chain 2: Generates structured citations for the answer ---\n",
        "\n",
        "cite_response_chain = (\n",
        "    {\n",
        "        # Pass the original, unformatted context through.\n",
        "        \"context\": itemgetter('context'),\n",
        "        # Pass the original question through.\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        # IMPORTANT: This chain requires the 'answer' generated by the previous chain.\n",
        "        \"answer\": itemgetter(\"answer\")\n",
        "    }\n",
        "    |\n",
        "    # Use the specialized citation prompt.\n",
        "    cite_prompt_template\n",
        "    |\n",
        "    # Use the structured output model to get a JSON object of citations.\n",
        "    structured_chatgpt\n",
        ")\n",
        "\n",
        "\n",
        "# --- Chain 3: The final orchestrator chain ---\n",
        "\n",
        "rag_chain_w_citations = (\n",
        "    # Step 1: Retrieval. Fetch context documents based on the question.\n",
        "    # Output of this step: {\"context\": [docs], \"question\": \"user_question\"}\n",
        "    {\n",
        "        \"context\": similarity_retriever,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    |\n",
        "    # Step 2: Generate Answer. Run the first chain to get the text answer and\n",
        "    # add it to the dictionary under the key \"answer\".\n",
        "    # Output of this step: {\"context\": [docs], \"question\": \"user_question\", \"answer\": \"text_answer\"}\n",
        "    RunnablePassthrough.assign(answer=rag_response_chain)\n",
        "    |\n",
        "    # Step 3: Generate Citations. Run the second chain using the output from the previous step.\n",
        "    # The result (structured citations) is added to the dictionary under the key \"citations\".\n",
        "    # Output of this step: {\"context\": ..., \"question\": ..., \"answer\": ..., \"citations\": ...}\n",
        "    RunnablePassthrough.assign(citations=cite_response_chain)\n",
        ")"
      ],
      "metadata": {
        "id": "qt0t3-8T4Doi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is machine learning\"\n",
        "result = rag_chain_w_citations.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9937c631-566a-4e7b-eb9d-0a58eeb42df1",
        "id": "Qt12nMvTjVp2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(id='23437141-e83c-4a58-b26d-df3ef3e2d68e', metadata={'id': '564928', 'source': 'Wikipedia', 'page': 1, 'title': 'Machine learning'}, page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'),\n",
              "  Document(id='e819102a-7ef6-411a-a919-210a86db4cb8', metadata={'page': 1, 'id': '359370', 'source': 'Wikipedia', 'title': 'Supervised learning'}, page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.'),\n",
              "  Document(id='09e90054-ded4-4823-b01b-999e4a96b03f', metadata={'page': 1, 'source': 'Wikipedia', 'title': 'Deep learning', 'id': '663523'}, page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.'),\n",
              "  Document(id='44039d8c-b2ef-4d08-bad1-ba0adfc8ac6a', metadata={'id': '6360', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial intelligence'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n",
              "  Document(id='ffc088e9-efaa-49e4-bd88-3da309c201b9', metadata={'page': 1, 'title': 'Artificial neural network', 'id': '44742', 'source': 'Wikipedia'}, page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.')],\n",
              " 'question': 'What is machine learning',\n",
              " 'answer': 'Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in the broader field of artificial intelligence (AI). \\n\\nMachine learning focuses on the study and construction of algorithms that can learn from and make predictions based on data. These algorithms operate by following programmed instructions while also being capable of making predictions or decisions based on the data they process. They build models from sample inputs, which allows them to function in scenarios where explicit algorithm design and programming are not feasible.\\n\\nSome common applications of machine learning include:\\n- Spam filtering\\n- Detection of network intruders or malicious insiders\\n- Optical character recognition (OCR)\\n- Search engines\\n- Computer vision\\n\\nOverall, machine learning enables systems to improve their performance on tasks over time as they are exposed to more data, making it a powerful tool in various domains.',\n",
              " 'citations': QuotedCitations(citations=[Citation(id='564928', source='Wikipedia', title='Machine learning', page=1, quotes='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'), Citation(id='6360', source='Wikipedia', title='Artificial intelligence', page=1, quotes='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition.')])}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['citations'].dict()['citations']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3fufsoAHiuD",
        "outputId": "27a7c0a1-6608-43a3-bd19-6d9fea5d6039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3486563740.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  result['citations'].dict()['citations']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': '564928',\n",
              "  'source': 'Wikipedia',\n",
              "  'title': 'Machine learning',\n",
              "  'page': 1,\n",
              "  'quotes': 'Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'},\n",
              " {'id': '6360',\n",
              "  'source': 'Wikipedia',\n",
              "  'title': 'Artificial intelligence',\n",
              "  'page': 1,\n",
              "  'quotes': 'Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# used mostly for nice display formatting, ignore if not needed\n",
        "def get_cited_context(result_obj):\n",
        "    # Dictionary to hold separate citation information for each unique source and title combination\n",
        "    source_with_citations = {}\n",
        "\n",
        "    def highlight_text(context, quote):\n",
        "        # Normalize whitespace and remove unnecessary punctuation\n",
        "        quote = re.sub(r'\\s+', ' ', quote).strip()\n",
        "        context = re.sub(r'\\s+', ' ', context).strip()\n",
        "\n",
        "        # Split quote into phrases, being careful with punctuation\n",
        "        phrases = [phrase.strip() for phrase in re.split(r'[.!?]', quote) if phrase.strip()]\n",
        "\n",
        "        highlighted_context = context\n",
        "\n",
        "        for phrase in phrases: # for each quoted phrase\n",
        "\n",
        "            # Create regex pattern to match cited phrases\n",
        "            # Escape special regex characters, but preserve word boundaries\n",
        "            escaped_phrase = re.escape(phrase)\n",
        "            # Create regex pattern that allows for slight variations\n",
        "            pattern = re.compile(r'\\b' + escaped_phrase + r'\\b', re.IGNORECASE)\n",
        "\n",
        "            # Replace all matched phrases with bolded version\n",
        "            highlighted_context = pattern.sub(lambda m: f\"**{m.group(0)}**\", highlighted_context)\n",
        "\n",
        "        return highlighted_context\n",
        "\n",
        "    # Process the citation data\n",
        "    for cite in result_obj['citations'].dict()['citations']:\n",
        "        cite_id = cite['id']\n",
        "        title = cite['title']\n",
        "        source = cite['source']\n",
        "        page = cite['page']\n",
        "        quote = cite['quotes']\n",
        "\n",
        "        # Check if the (source, title) key exists, and initialize if it doesn't\n",
        "        if (source, title) not in source_with_citations:\n",
        "            source_with_citations[(source, title)] = {\n",
        "                'title': title,\n",
        "                'source': source,\n",
        "                'citations': []\n",
        "            }\n",
        "\n",
        "        # Find or create the citation entry for this unique (id, page) combination\n",
        "        citation_entry = next(\n",
        "            (c for c in source_with_citations[(source, title)]['citations'] if c['id'] == cite_id and c['page'] == page),\n",
        "            None\n",
        "        )\n",
        "        if citation_entry is None:\n",
        "            citation_entry = {'id': cite_id, 'page': page, 'quote': [quote], 'context': None}\n",
        "            source_with_citations[(source, title)]['citations'].append(citation_entry)\n",
        "        else:\n",
        "            citation_entry['quote'].append(quote)\n",
        "\n",
        "    # Process context data\n",
        "    for context in result_obj['context']:\n",
        "        context_id = context.metadata['id']\n",
        "        context_page = context.metadata['page']\n",
        "        source = context.metadata['source']\n",
        "        title = context.metadata['title']\n",
        "        page_content = context.page_content\n",
        "\n",
        "        # Match the context to the correct citation entry by source, title, id, and page\n",
        "        if (source, title) in source_with_citations:\n",
        "            for citation in source_with_citations[(source, title)]['citations']:\n",
        "                if citation['id'] == context_id and citation['page'] == context_page:\n",
        "                    # Apply highlighting for each quote in the citation's quote list\n",
        "                    highlighted_content = page_content\n",
        "                    for quote in citation['quote']:\n",
        "                        highlighted_content = highlight_text(highlighted_content, quote)\n",
        "                    citation['context'] = highlighted_content\n",
        "\n",
        "    # Convert the dictionary to a list of dictionaries for separate entries\n",
        "    final_result_list = [\n",
        "        {\n",
        "            'title': details['title'],\n",
        "            'source': details['source'],\n",
        "            'citations': details['citations']\n",
        "        }\n",
        "        for details in source_with_citations.values()\n",
        "    ]\n",
        "\n",
        "    return final_result_list\n"
      ],
      "metadata": {
        "id": "docbPBPDxSVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_cited_context(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq8XyKnlJb4i",
        "outputId": "d2d730d4-d60f-4b63-bc48-91fa18c8e8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  for cite in result_obj['citations'].dict()['citations']:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Machine learning',\n",
              "  'source': 'Wikipedia',\n",
              "  'citations': [{'id': '564928',\n",
              "    'page': 1,\n",
              "    'quote': ['Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'],\n",
              "    'context': 'Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. **They build a model from sample inputs**. **Machine learning is done where designing and programming explicit algorithms cannot be done**. **Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision**.'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_results(result_obj):\n",
        "    print('Query:')\n",
        "    display(Markdown(result_obj['question']))\n",
        "    print()\n",
        "    print('Response:')\n",
        "    display(Markdown(result_obj['answer']))\n",
        "    print('='*50)\n",
        "    print('Sources:')\n",
        "    cited_context = get_cited_context(result_obj)\n",
        "    for source in cited_context:\n",
        "        print('Title:', source['title'], ' ', 'Source:', source['source'])\n",
        "        print('Citations:')\n",
        "        for citation in source['citations']:\n",
        "            print('ID:', citation['id'], ' ', 'Page:', citation['page'])\n",
        "            print('Cited Quotes:')\n",
        "            display(Markdown('*'+' '.join(citation['quote'])+'*'))\n",
        "            print('Cited Context:')\n",
        "            display(Markdown(citation['context']))\n",
        "            print()\n"
      ],
      "metadata": {
        "id": "aChUXboG903B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_results(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "pG6fcxAE3I3G",
        "outputId": "6b10e646-7da7-48bf-d85b-4e39ac2655d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "What is machine learning"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in the broader field of artificial intelligence (AI). \n\nMachine learning focuses on the study and construction of algorithms that can learn from and make predictions based on data. These algorithms operate by following programmed instructions but also have the capability to make predictions or decisions based on the data they process. They build models from sample inputs, which allows them to function in scenarios where traditional programming methods are insufficient. \n\nSome common applications of machine learning include:\n- Spam filtering\n- Detection of network intruders or malicious insiders\n- Optical character recognition (OCR)\n- Search engines\n- Computer vision\n\nOverall, machine learning enables systems to improve their performance on tasks over time as they are exposed to more data, making it a powerful tool in various technological applications."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Sources:\n",
            "Title: Machine learning   Source: Wikipedia\n",
            "Citations:\n",
            "ID: 564928   Page: 1\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  for cite in result_obj['citations'].dict()['citations']:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. **They build a model from sample inputs**. **Machine learning is done where designing and programming explicit algorithms cannot be done**. **Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision**."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is AI, ML and DL?\"\n",
        "result = rag_chain_w_citations.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rgTlW5hg_d0x",
        "outputId": "fdcdfc14-9fbb-4877-ea77-1757e9c4e814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "What is AI, ML and DL?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Artificial Intelligence (AI)**: AI is defined as the ability of a computer program or machine to think and learn. It is also a field of study aimed at making computers \"smart,\" allowing them to operate independently without being explicitly programmed with commands. The term was coined by John McCarthy in 1955. AI encompasses systems that can interpret external data, learn from it, and adapt to achieve specific goals. As technology advances, tasks once considered to require intelligence, such as optical character recognition, are no longer classified as AI but rather as routine technologies.\n\n**Machine Learning (ML)**: ML is a subfield of computer science that provides computers the ability to learn from data without being explicitly programmed. This concept emerged from the broader field of artificial intelligence. Machine learning involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. It is particularly useful in scenarios where traditional programming is impractical. Examples of machine learning applications include spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.\n\n**Deep Learning (DL)**: DL is a specialized form of machine learning that primarily utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). It can involve unsupervised, semi-supervised, or supervised learning sessions. Deep learning is particularly effective for complex tasks such as speech recognition, image understanding, and handwriting recognition, which are challenging for computers. The architecture of deep learning models is inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Sources:\n",
            "Title: Artificial intelligence   Source: Wikipedia\n",
            "Citations:\n",
            "ID: 6360   Page: 1\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  for cite in result_obj['citations'].dict()['citations']:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn**. It is also a field of study which tries to make computers \"smart\". **They work on their own without being encoded with commands**. **John McCarthy came up with the name \"Artificial Intelligence\" in 1955**. **In general use, the term \"artificial intelligence\" means a programme which mimics human cognition**. **At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do**. **Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation**. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title: Machine learning   Source: Wikipedia\n",
            "Citations:\n",
            "ID: 564928   Page: 1\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title: Deep learning   Source: Wikipedia\n",
            "Citations:\n",
            "ID: 663523   Page: 1\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks**. **As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised**. **In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer**. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is Machine learning related to supervised learning and clustering?\"\n",
        "result = rag_chain_w_citations.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zho3RfTQ-_WU",
        "outputId": "79d50d2c-cc61-486c-b6a6-a49b2a01e2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "How is Machine learning related to supervised learning and clustering?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning is a broad field that encompasses various techniques and methodologies for enabling computers to learn from data. Two important concepts within machine learning are supervised learning and clustering.\n\n### Supervised Learning\n- **Definition**: Supervised learning is a specific type of machine learning where the model is trained on labeled data. This means that the training dataset includes both the input data and the corresponding correct outputs (labels).\n- **Process**: The system infers a function from this labeled training data, learning how to map inputs to the correct outputs. The results of the training are known beforehand, allowing the system to learn to produce a \"classifier\" that can make predictions on new, unseen data.\n- **Inductive Reasoning**: Supervised learning typically employs inductive reasoning to generalize from the training data to make predictions about new data.\n\n### Clustering\n- **Definition**: Clustering, or cluster analysis, is a type of data analysis that involves grouping a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups.\n- **Application**: Clustering is commonly used in data mining and is a form of unsupervised learning, where the model learns patterns from data without labeled outputs. Unlike supervised learning, clustering does not rely on predefined labels to guide the learning process.\n\n### Relationship Between Machine Learning, Supervised Learning, and Clustering\n- **Machine Learning as an Umbrella**: Machine learning serves as the overarching field that includes various learning paradigms, including both supervised learning and clustering.\n- **Different Approaches**: While supervised learning focuses on learning from labeled data to make predictions, clustering is concerned with discovering inherent groupings in data without prior labels. Both approaches are essential for different types of tasks within machine learning, showcasing the versatility of the field.\n\nIn summary, machine learning encompasses both supervised learning, which relies on labeled data for training, and clustering, which identifies patterns in unlabeled data. Each serves distinct purposes and utilizes different methodologies within the broader context of machine learning."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Sources:\n",
            "Title: Supervised learning   Source: Wikipedia\n",
            "Citations:\n",
            "ID: 359370   Page: 1\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  for cite in result_obj['citations'].dict()['citations']:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**In machine learning, supervised learning is the task of inferring a function from labelled training data**. **The results of the training are known beforehand, the system simply learns how to get to these results correctly**. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title: Machine learning   Source: Wikipedia\n",
            "Citations:\n",
            "ID: 564928   Page: 1\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Machine learning gives computers the ability to learn without being explicitly programmed. It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Machine learning gives computers the ability to learn without being explicitly programmed** (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title: Cluster analysis   Source: Wikipedia\n",
            "Citations:\n",
            "ID: 593732   Page: 1\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Clustering or cluster analysis is a type of data analysis**. **The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way**. This is a common task in data mining."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between transformers and vision transformers?\"\n",
        "result = rag_chain_w_citations.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vy2zTEfAFxBo",
        "outputId": "f459d53a-401d-4cb5-9ec1-4b3daeecc6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "What is the difference between transformers and vision transformers?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The difference between transformers and vision transformers (ViTs) primarily lies in their application and the way they process input data.\n\n### Transformers\n- **Application**: Transformers were originally designed for natural language processing (NLP) tasks. They excel in handling sequential data, where the input is typically a sequence of tokens (words).\n- **Input Processing**: In a standard transformer, the input is a 1D sequence of token embeddings. Each token is processed through layers of self-attention and feedforward networks, allowing the model to capture relationships between tokens regardless of their position in the sequence.\n\n### Vision Transformers (ViTs)\n- **Application**: Vision transformers adapt the transformer architecture for image classification tasks. They treat images as sequences of patches rather than as a whole.\n- **Input Processing**: In ViTs, an image is divided into fixed-size patches, which are then flattened and linearly embedded into a sequence of vectors. This sequence is fed into a standard transformer encoder. The patches are treated similarly to tokens in NLP, allowing the model to leverage the self-attention mechanism to integrate information across the entire image.\n- **Inductive Bias**: Unlike convolutional neural networks (CNNs), which have built-in inductive biases such as locality and translation equivariance, ViTs have much less image-specific inductive bias. They rely on the model to learn spatial relationships from the data itself, which can be advantageous when trained on large datasets.\n\n### Summary\nIn summary, while both transformers and vision transformers utilize the same underlying architecture, their differences lie in their input formats and the specific tasks they are designed to handle. Transformers are optimized for sequential data in NLP, whereas vision transformers adapt this approach for image data by treating image patches as sequences, allowing them to perform well in image classification tasks when trained on large datasets."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Sources:\n",
            "Title: vision_transformer.pdf   Source: ./rag_docs/vision_transformer.pdf\n",
            "Citations:\n",
            "ID: 561364f3-23f5-428d-b43a-6171c2586694   Page: 0\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  for cite in result_obj['citations'].dict()['citations']:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*It highlights the limitations of convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks when pre-trained on large datasets, while requiring fewer computational resources.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. **It highlights the limitations of convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks when pre-trained on large datasets, while requiring fewer computational resources**. Published as a conference paper at ICLR 2021 AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗, Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,† ∗equal technical contribution, †equal advising Google Research, Brain Team {adosovitskiy, neilhoulsby}@google.com ABSTRACT While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classiﬁcation tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring sub- stantially fewer computational resources to train.1 1 INTRODUCTION Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks to Transformers’ computational efﬁciency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the models and datasets growing, there is still no sign of saturating performance. In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet- like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Trans- former. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classiﬁcation in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these mod- els yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases 1Fine-tuning code and pre-trained models are available at https://github.com/ google-research/vision_transformer 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ID: 39fe003d-429a-4e73-9262-f375cc845fa1   Page: 1\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ID: 161ff54f-640b-40ee-b1dc-7724bce5522b   Page: 2\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*To handle 2D images, we reshape the image x ∈RH×W ×C into a sequence of flattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ID: 5091e5e8-6492-4e8e-8de3-40c4b90b5142   Page: 3\n",
            "Cited Quotes:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cited Context:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CoTGhQSA3pod"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}